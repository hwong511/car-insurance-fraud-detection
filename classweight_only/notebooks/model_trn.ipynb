{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78eeda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataset Class for PyTorch (No Augmentation)\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTImageProcessor\n",
    "\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, image_dir, processor, label_map, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.processor = processor\n",
    "\n",
    "        for label_name in os.listdir(image_dir):\n",
    "            class_dir = os.path.join(image_dir, label_name)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.image_paths.append(os.path.join(class_dir, fname))\n",
    "                    self.labels.append(label_map[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
    "        item = {key: val.squeeze(0) for key, val in processed.items()}\n",
    "        item[\"labels\"] = label\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8057f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataset Class with Augmentation for Fraud Images\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Define transforms for augmentation\n",
    "fraud_augmentation = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class FraudDatasetWithAugmentation(Dataset):\n",
    "    def __init__(self, image_dir, processor, label_map, augment_fraud=False):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.processor = processor\n",
    "        self.augment_fraud = augment_fraud\n",
    "\n",
    "        for label_name in os.listdir(image_dir):\n",
    "            class_dir = os.path.join(image_dir, label_name)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.image_paths.append(os.path.join(class_dir, fname))\n",
    "                    self.labels.append(label_map[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Only augment Fraud images during training\n",
    "        if self.augment_fraud and label == 1:\n",
    "            image = fraud_augmentation(image)\n",
    "\n",
    "            # Re-process to match expected ViT input\n",
    "            processed = self.processor(images=image, return_tensors=\"pt\", do_rescale=False)\n",
    "        else:\n",
    "            processed = self.processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in processed.items()}\n",
    "        item[\"labels\"] = label\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1638f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Class Weights for Imbalanced Dataset\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y=[0]*4000 + [1]*160\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c038d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# Define class names → labels\n",
    "label_map = {\"Non-Fraud\": 0, \"Fraud\": 1}\n",
    "\n",
    "# Load processor\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FraudDataset(\"data/train\", processor, label_map)\n",
    "val_dataset = FraudDataset(\"data/val\", processor, label_map)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,        # Set to 0 to disable multiprocessing\n",
    "    pin_memory=False      # Disable pin_memory\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,        # Set to 0 to disable multiprocessing  \n",
    "    pin_memory=False      # Disable pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3870f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training for 10 epochs\n",
      "Total training steps: 1300\n",
      "Class weights - Non-fraud: 0.520, Fraud: 13.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [TRAIN]: 100%|█| 130/130 [17:37<00:00,  8.14s/batch, Loss=0.1273, Avg=0.7869, LR=1.9e-05,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [TRAIN]: 100%|█| 130/130 [11:53<00:00,  5.49s/batch, Loss=0.0106, Avg=0.0279, LR=1.7e-05,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training Loss: 0.2799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [TRAIN]: 100%|█| 130/130 [10:52<00:00,  5.02s/batch, Loss=0.0040, Avg=0.0475, LR=1.5e-05,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training Loss: 0.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [TRAIN]: 100%|█| 130/130 [11:24<00:00,  5.27s/batch, Loss=0.0014, Avg=0.0057, LR=1.3e-05,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training Loss: 0.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [TRAIN]: 100%|█| 130/130 [11:30<00:00,  5.31s/batch, Loss=0.0005, Avg=0.0007, LR=1.1e-05,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [TRAIN]: 100%|█| 130/130 [11:20<00:00,  5.23s/batch, Loss=0.0003, Avg=0.0004, LR=8.7e-06,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [TRAIN]: 100%|█| 130/130 [14:03<00:00,  6.49s/batch, Loss=0.0002, Avg=0.0004, LR=6.5e-06,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [TRAIN]: 100%|█| 130/130 [14:58<00:00,  6.91s/batch, Loss=0.0001, Avg=0.0003, LR=4.3e-06,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [TRAIN]: 100%|█| 130/130 [11:11<00:00,  5.16s/batch, Loss=0.0002, Avg=0.0003, LR=2.2e-06,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [TRAIN]: 100%|█| 130/130 [11:48<00:00,  5.45s/batch, Loss=0.0003, Avg=0.0003, LR=0.0e+00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTForImageClassification\n",
    "import time\n",
    "\n",
    "# Configurations\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Loading Model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=len(label_map),\n",
    "    id2label={v: k for k, v in label_map.items()},\n",
    "    label2id=label_map,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training Loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_auc = 0.0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Class weights - Non-fraud: {weights[0]:.3f}, Fraud: {weights[1]:.3f}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # ===== TRAINING PHASE =====\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    train_pbar = tqdm(\n",
    "        train_loader, \n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs} [TRAIN]\",\n",
    "        leave=True,\n",
    "        ncols=100,\n",
    "        unit=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_losses = []\n",
    "    for batch_idx, batch in enumerate(train_pbar):\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_losses.append(batch_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)\n",
    "            preds = torch.argmax(probs, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate batch time and ETA\n",
    "        batch_time = time.time() - batch_start_time\n",
    "\n",
    "        # Update progress bar with detailed info\n",
    "        train_pbar.set_postfix({\n",
    "            'Loss': f\"{batch_loss:.4f}\",\n",
    "            'Avg': f\"{np.mean(batch_losses[-10:]):.4f}\",  # Running average of last 10\n",
    "            'LR': f\"{scheduler.get_last_lr()[0]:.1e}\",\n",
    "            'Time': f\"{batch_time:.2f}s\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bda66816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as vit_base_pre_tuning.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model checkpoint\n",
    "torch.save(model.state_dict(), \"vit_base_pre_tuning.pth\")\n",
    "print(\"Model saved as vit_base_pre_tuning.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6043e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Validation Loss: 0.0861\n"
     ]
    }
   ],
   "source": [
    "# Validation Loop\n",
    "\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "val_preds = []\n",
    "val_labels = []\n",
    "val_probs = []\n",
    "\n",
    "val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", leave=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "        val_probs.extend(probs[:, 1].cpu().numpy())  # Probability of the positive class\n",
    "\n",
    "        # Update progress bar\n",
    "        val_pbar.set_postfix({\n",
    "            'val_loss': f\"{loss.item():.4f}\"\n",
    "        })\n",
    "\n",
    "avg_val_loss = total_val_loss / len(val_loader)\n",
    "val_losses.append(avg_val_loss)\n",
    "print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c733a1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Validation Loss: 0.0861, Validation AUC: 0.9840\n",
      "\n",
      "Classification Report:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Fraud       0.99      1.00      0.99      1000\n",
      "       Fraud       0.91      0.80      0.85        40\n",
      "\n",
      "    accuracy                           0.99      1040\n",
      "   macro avg       0.95      0.90      0.92      1040\n",
      "weighted avg       0.99      0.99      0.99      1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUC\n",
    "val_auc = roc_auc_score(val_labels, val_probs)\n",
    "\n",
    "print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}, Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\" \")\n",
    "print(classification_report(val_labels, val_preds, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec0c3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving (Approach 1)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'val_auc': best_val_auc\n",
    "}, \"best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
